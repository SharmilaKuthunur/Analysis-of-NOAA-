{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Preprocessing\n",
    "\n",
    "This section contains the following: \n",
    "\n",
    "    1.1. Overview of the dataset\n",
    "    1.2. Addressing missing values\n",
    "    1.3. Filtering required columns\n",
    "    1.4. Adding required columns\n",
    "    1.5. Offsetting\n",
    "    1.6. Downloading preprocessing dataset as a CSV file\n",
    "    1.7. Insights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by importing the required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, the downloaded dataset *Analysis* is read into a Pandas DataFrame *preprocessing_dataset*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing_dataset = pd.read_csv(\"C:\\\\Users\\\\Sharmila\\\\Documents\\\\Earthquake Project\\\\Earthquake dataset.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=black>1.1. Overview of the Dataset</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The columns in the dataset, their types, index values, first and last five rows is displayed. The describe() function is used to provide a quick statistical overview of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Year', 'Mo', 'Dy', 'Hr', 'Mn', 'Sec', 'Tsu', 'Vol', 'Addl', 'Name',\n",
      "       'Latitude', 'Longitude', 'Focal', 'Mag', 'MMI Int', 'Num', 'De',\n",
      "       'Num.1', 'De.1', '$Mill', 'De.2', 'Num.2', 'De.3', 'Num.3', 'De.4',\n",
      "       'Unnamed: 25'],\n",
      "      dtype='object')\n",
      "26\n"
     ]
    }
   ],
   "source": [
    "# The names of the columns\n",
    "\n",
    "columns_dataset = preprocessing_dataset.columns\n",
    "print(columns_dataset)\n",
    "print(len(columns_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Year             int64\n",
      "Mo             float64\n",
      "Dy             float64\n",
      "Hr             float64\n",
      "Mn             float64\n",
      "Sec            float64\n",
      "Tsu             object\n",
      "Vol             object\n",
      "Addl            object\n",
      "Name            object\n",
      "Latitude       float64\n",
      "Longitude      float64\n",
      "Focal          float64\n",
      "Mag            float64\n",
      "MMI Int        float64\n",
      "Num            float64\n",
      "De             float64\n",
      "Num.1          float64\n",
      "De.1           float64\n",
      "$Mill          float64\n",
      "De.2           float64\n",
      "Num.2          float64\n",
      "De.3           float64\n",
      "Num.3          float64\n",
      "De.4           float64\n",
      "Unnamed: 25    float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# The data type of each column\n",
    "\n",
    "print(preprocessing_dataset.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0,    1,    2, ..., 6141, 6142, 6143], dtype=int64)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Next, the indexes associated with the dataframe are checked\n",
    "\n",
    "preprocessing_dataset.index.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The top five and bottom five rows are displayed below. It is seen that each observation forms a row, is defined by an index, and attributes of those observations are displayed in the columns of the DataFrame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Year  Mo  Dy  Hr  Mn  Sec  Tsu  Vol Addl                              Name  \\\n",
      "0 -2150 NaN NaN NaN NaN  NaN  NaN  NaN    *      JORDAN: BAB-A-DARAA,AL-KARAK   \n",
      "1 -2000 NaN NaN NaN NaN  NaN  Tsu  NaN    *                     SYRIA: UGARIT   \n",
      "2 -2000 NaN NaN NaN NaN  NaN  NaN  NaN    *                   TURKMENISTAN: W   \n",
      "3 -1610 NaN NaN NaN NaN  NaN  Tsu  Vol    *  GREECE: THERA ISLAND (SANTORINI)   \n",
      "4 -1566 NaN NaN NaN NaN  NaN  NaN  NaN    *           ISRAEL: ARIHA (JERICHO)   \n",
      "\n",
      "   ...   De  Num.1  De.1  $Mill  De.2  Num.2  De.3  Num.3  De.4  Unnamed: 25  \n",
      "0  ...  NaN    NaN   NaN    NaN   3.0    NaN   NaN    NaN   NaN          NaN  \n",
      "1  ...  3.0    NaN   NaN    NaN   NaN    NaN   NaN    NaN   NaN          NaN  \n",
      "2  ...  1.0    NaN   NaN    NaN   1.0    NaN   1.0    NaN   NaN          NaN  \n",
      "3  ...  NaN    NaN   NaN    NaN   NaN    NaN   NaN    NaN   NaN          NaN  \n",
      "4  ...  NaN    NaN   NaN    NaN   3.0    NaN   NaN    NaN   NaN          NaN  \n",
      "\n",
      "[5 rows x 26 columns]\n"
     ]
    }
   ],
   "source": [
    "# First five rows\n",
    "\n",
    "print(preprocessing_dataset.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Year   Mo    Dy    Hr    Mn   Sec  Tsu  Vol Addl  \\\n",
      "6139  2019  9.0  24.0  11.0   1.0  55.0  NaN  NaN    *   \n",
      "6140  2019  9.0  25.0  23.0  46.0  44.0  NaN  NaN    *   \n",
      "6141  2019  9.0  26.0  10.0  59.0  26.0  NaN  NaN    *   \n",
      "6142  2019  9.0  26.0  16.0  36.0  18.0  NaN  NaN    *   \n",
      "6143  2019  9.0  29.0  15.0  57.0  53.0  NaN  NaN    *   \n",
      "\n",
      "                           Name  ...   De  Num.1  De.1  $Mill  De.2  Num.2  \\\n",
      "6139  PAKISTAN: MIRPUR DISTRICT  ...  1.0  746.0   3.0    NaN   2.0  135.0   \n",
      "6140   INDONESIA: MALUKU: AMBON  ...  1.0  179.0   3.0    NaN   3.0    NaN   \n",
      "6141           TURKEY: ISTANBUL  ...  1.0   34.0   1.0    NaN   2.0    NaN   \n",
      "6142       CHILE: SOUTH CENTRAL  ...  1.0    NaN   NaN    NaN   NaN    NaN   \n",
      "6143          CHILE: CONCEPCION  ...  1.0    NaN   NaN    NaN   NaN    NaN   \n",
      "\n",
      "      De.3   Num.3  De.4  Unnamed: 25  \n",
      "6139   3.0   319.0   3.0          NaN  \n",
      "6140   1.0  2675.0   2.0          NaN  \n",
      "6141   NaN   473.0   3.0          NaN  \n",
      "6142   NaN     NaN   NaN          NaN  \n",
      "6143   NaN     NaN   NaN          NaN  \n",
      "\n",
      "[5 rows x 26 columns]\n"
     ]
    }
   ],
   "source": [
    "# Last five rows\n",
    "\n",
    "print(preprocessing_dataset.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Year           Mo           Dy           Hr          Mn  \\\n",
      "count   6144.000000  5739.000000  5587.000000  4113.000000  3908.00000   \n",
      "unique          NaN          NaN          NaN          NaN         NaN   \n",
      "top             NaN          NaN          NaN          NaN         NaN   \n",
      "freq            NaN          NaN          NaN          NaN         NaN   \n",
      "mean    1804.691243     6.505663    15.721675    11.304644    28.86131   \n",
      "std      376.404999     3.449445     8.749653     7.032811    17.14624   \n",
      "min    -2150.000000     1.000000     1.000000     0.000000     0.00000   \n",
      "25%     1820.000000     4.000000     8.000000     5.000000    14.75000   \n",
      "50%     1928.000000     7.000000    16.000000    11.000000    30.00000   \n",
      "75%     1988.000000     9.000000    23.000000    17.000000    44.00000   \n",
      "max     2019.000000    12.000000    31.000000    23.000000    59.00000   \n",
      "\n",
      "                Sec   Tsu  Vol  Addl                    Name  ...  \\\n",
      "count   2791.000000  1853   64  6144                    6143  ...   \n",
      "unique          NaN     1    1     1                    3790  ...   \n",
      "top             NaN   Tsu  Vol     *  CHINA: YUNNAN PROVINCE  ...   \n",
      "freq            NaN  1853   64  6144                      79  ...   \n",
      "mean      29.663633   NaN  NaN   NaN                     NaN  ...   \n",
      "std       17.176807   NaN  NaN   NaN                     NaN  ...   \n",
      "min        0.100000   NaN  NaN   NaN                     NaN  ...   \n",
      "25%       14.550000   NaN  NaN   NaN                     NaN  ...   \n",
      "50%       29.400000   NaN  NaN   NaN                     NaN  ...   \n",
      "75%       44.450000   NaN  NaN   NaN                     NaN  ...   \n",
      "max       59.900000   NaN  NaN   NaN                     NaN  ...   \n",
      "\n",
      "                 De          Num.1         De.1          $Mill         De.2  \\\n",
      "count   2510.000000    1212.000000  1397.000000     496.000000  4387.000000   \n",
      "unique          NaN            NaN          NaN            NaN          NaN   \n",
      "top             NaN            NaN          NaN            NaN          NaN   \n",
      "freq            NaN            NaN          NaN            NaN          NaN   \n",
      "mean       2.019124    2215.919142     1.957767    1245.874780     2.265785   \n",
      "std        1.156726   26612.926726     1.080734    6820.271167     0.955949   \n",
      "min        0.000000       1.000000     1.000000       0.013000     1.000000   \n",
      "25%        1.000000      10.000000     1.000000       3.850000     1.000000   \n",
      "50%        1.000000      40.000000     1.000000      21.400000     2.000000   \n",
      "75%        3.000000     200.000000     3.000000     196.250000     3.000000   \n",
      "max        4.000000  799000.000000     4.000000  100000.000000     4.000000   \n",
      "\n",
      "               Num.2         De.3         Num.3        De.4  Unnamed: 25  \n",
      "count   7.690000e+02  1669.000000  4.690000e+02  894.000000    61.000000  \n",
      "unique           NaN          NaN           NaN         NaN          NaN  \n",
      "top              NaN          NaN           NaN         NaN          NaN  \n",
      "freq             NaN          NaN           NaN         NaN          NaN  \n",
      "mean    1.785742e+04     2.689635  2.564698e+04    2.517897    11.754098  \n",
      "std     1.993090e+05     1.058037  2.550184e+05    1.122514    12.492205  \n",
      "min     1.000000e+00     0.000000  1.000000e+00    1.000000     1.000000  \n",
      "25%     6.500000e+01     2.000000  8.700000e+01    1.000000     2.000000  \n",
      "50%     5.120000e+02     3.000000  6.140000e+02    3.000000     7.000000  \n",
      "75%     4.000000e+03     3.000000  3.487000e+03    3.000000    20.000000  \n",
      "max     5.360000e+06     4.000000  5.360000e+06    4.000000    54.000000  \n",
      "\n",
      "[11 rows x 26 columns]\n"
     ]
    }
   ],
   "source": [
    "# The basic statistical overview of the dataset is provided using Pandas describe()\n",
    "\n",
    "print(preprocessing_dataset.describe(include = 'all'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Addressing Missing Values\n",
    "\n",
    "The missing values in *Name, Latitude, Longitude*, and *Magnitude* are explored in detail. A question mark(?) is used to replace the missing values in other variables.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Year              0\n",
      "Mo              405\n",
      "Dy              557\n",
      "Hr             2031\n",
      "Mn             2236\n",
      "Sec            3353\n",
      "Tsu            4291\n",
      "Vol            6080\n",
      "Addl              0\n",
      "Name              1\n",
      "Latitude         54\n",
      "Longitude        50\n",
      "Focal          2951\n",
      "Mag            1783\n",
      "MMI Int        3379\n",
      "Num            4104\n",
      "De             3634\n",
      "Num.1          4932\n",
      "De.1           4747\n",
      "$Mill          5648\n",
      "De.2           1757\n",
      "Num.2          5375\n",
      "De.3           4475\n",
      "Num.3          5675\n",
      "De.4           5250\n",
      "Unnamed: 25    6083\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# The null values in the dataset is viewed: \n",
    "\n",
    "print(preprocessing_dataset.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1. Name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There exists one missing entry in *Name*. The row containing missing value is retrieved. Based on the latitude and longitude of mentioned in that row, the entire dataset is searched to find a similar entry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([2028], dtype=int64),)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Accessing index of the missing entry in the Name column\n",
    "\n",
    "np.where(pd.isnull(preprocessing_dataset['Name']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Year           1868\n",
       "Mo               10\n",
       "Dy               18\n",
       "Hr               12\n",
       "Mn               35\n",
       "Sec             NaN\n",
       "Tsu             Tsu\n",
       "Vol             NaN\n",
       "Addl              *\n",
       "Name            NaN\n",
       "Latitude      -40.2\n",
       "Longitude       173\n",
       "Focal            12\n",
       "Mag             7.6\n",
       "MMI Int         NaN\n",
       "Num             NaN\n",
       "De              NaN\n",
       "Num.1           NaN\n",
       "De.1            NaN\n",
       "$Mill           NaN\n",
       "De.2            NaN\n",
       "Num.2           NaN\n",
       "De.3            NaN\n",
       "Num.3           NaN\n",
       "De.4            NaN\n",
       "Unnamed: 25     NaN\n",
       "Name: 2028, dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Accessing the entire row of index 202\n",
    "\n",
    "preprocessing_dataset.iloc[2028]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Year    Mo    Dy    Hr    Mn  Sec  Tsu  Vol Addl Name  ...  De  Num.1  \\\n",
      "2028  1868  10.0  18.0  12.0  35.0  NaN  Tsu  NaN    *  NaN  ... NaN    NaN   \n",
      "\n",
      "      De.1  $Mill  De.2  Num.2  De.3  Num.3  De.4  Unnamed: 25  \n",
      "2028   NaN    NaN   NaN    NaN   NaN    NaN   NaN          NaN  \n",
      "\n",
      "[1 rows x 26 columns]\n"
     ]
    }
   ],
   "source": [
    "# Finding similar rows with latitude -40.2 and longitude 173\n",
    "\n",
    "same_latitude = preprocessing_dataset[preprocessing_dataset['Latitude'] == -40.2] \n",
    "print(same_latitude)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Name column is NaN as well. Google Maps is used to find the exact location to be Cook Strait in New Zealand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assigning 'NEW ZEALAND: COOK STRAIT' to the missing value in 'Name'\n",
    "\n",
    "preprocessing_dataset[\"Name\"].fillna(\"NEW ZEALAND: COOK STRAIT\", inplace = True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# Verifying if the null value still exists\n",
    "\n",
    "print(preprocessing_dataset.Name.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2. Latitude and Longitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54\n"
     ]
    }
   ],
   "source": [
    "# Calculating the total number of missing latitudes\n",
    "\n",
    "print(preprocessing_dataset.Latitude.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n"
     ]
    }
   ],
   "source": [
    "# Calculating the total number of missing longitudes\n",
    "\n",
    "print(preprocessing_dataset.Longitude.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reasons behind the missing values is not yet understood. NOAA mentions that the unknown latitudes and longitudes are represented by '0.0'. Since there are 54 unknown latitudes and 50 unknown longitudes still present, all of them are replaced by '0.0'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing missing latitude values with 0.0\n",
    "\n",
    "preprocessing_dataset[\"Latitude\"].fillna(\"0.0\", inplace = True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing missing longitude values with 0.0\n",
    "\n",
    "preprocessing_dataset[\"Longitude\"].fillna(\"0.0\", inplace = True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verifying that all the null values in latitude column are replaced with 0.0\n",
    "\n",
    "preprocessing_dataset.Latitude.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verifying that all the null values in longitude column are replaced with 0.0\n",
    "\n",
    "preprocessing_dataset.Longitude.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.3. Other variables\n",
    "\n",
    "Not enough is known about the missing values in other variables. These empty entries are replaced with a question mark(?) for the time being."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Year              0\n",
       "Mo              405\n",
       "Dy              557\n",
       "Hr             2031\n",
       "Mn             2236\n",
       "Sec            3353\n",
       "Tsu            4291\n",
       "Vol            6080\n",
       "Addl              0\n",
       "Name              0\n",
       "Latitude          0\n",
       "Longitude         0\n",
       "Focal          2951\n",
       "Mag            1783\n",
       "MMI Int        3379\n",
       "Num            4104\n",
       "De             3634\n",
       "Num.1          4932\n",
       "De.1           4747\n",
       "$Mill          5648\n",
       "De.2           1757\n",
       "Num.2          5375\n",
       "De.3           4475\n",
       "Num.3          5675\n",
       "De.4           5250\n",
       "Unnamed: 25    6083\n",
       "dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculating the number of missing values in the dataset\n",
    "\n",
    "preprocessing_dataset.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Year Mo  Dy  Hr  Mn Sec  Tsu  Vol Addl  \\\n",
      "0    -2150  ?   ?   ?   ?   ?    ?    ?    *   \n",
      "1    -2000  ?   ?   ?   ?   ?  Tsu    ?    *   \n",
      "2    -2000  ?   ?   ?   ?   ?    ?    ?    *   \n",
      "3    -1610  ?   ?   ?   ?   ?  Tsu  Vol    *   \n",
      "4    -1566  ?   ?   ?   ?   ?    ?    ?    *   \n",
      "...    ... ..  ..  ..  ..  ..  ...  ...  ...   \n",
      "6139  2019  9  24  11   1  55    ?    ?    *   \n",
      "6140  2019  9  25  23  46  44    ?    ?    *   \n",
      "6141  2019  9  26  10  59  26    ?    ?    *   \n",
      "6142  2019  9  26  16  36  18    ?    ?    *   \n",
      "6143  2019  9  29  15  57  53    ?    ?    *   \n",
      "\n",
      "                                  Name  ... De Num.1 De.1 $Mill De.2 Num.2  \\\n",
      "0         JORDAN: BAB-A-DARAA,AL-KARAK  ...  ?     ?    ?     ?    3     ?   \n",
      "1                        SYRIA: UGARIT  ...  3     ?    ?     ?    ?     ?   \n",
      "2                      TURKMENISTAN: W  ...  1     ?    ?     ?    1     ?   \n",
      "3     GREECE: THERA ISLAND (SANTORINI)  ...  ?     ?    ?     ?    ?     ?   \n",
      "4              ISRAEL: ARIHA (JERICHO)  ...  ?     ?    ?     ?    3     ?   \n",
      "...                                ...  ... ..   ...  ...   ...  ...   ...   \n",
      "6139         PAKISTAN: MIRPUR DISTRICT  ...  1   746    3     ?    2   135   \n",
      "6140          INDONESIA: MALUKU: AMBON  ...  1   179    3     ?    3     ?   \n",
      "6141                  TURKEY: ISTANBUL  ...  1    34    1     ?    2     ?   \n",
      "6142              CHILE: SOUTH CENTRAL  ...  1     ?    ?     ?    ?     ?   \n",
      "6143                 CHILE: CONCEPCION  ...  1     ?    ?     ?    ?     ?   \n",
      "\n",
      "     De.3 Num.3 De.4 Unnamed: 25  \n",
      "0       ?     ?    ?           ?  \n",
      "1       ?     ?    ?           ?  \n",
      "2       1     ?    ?           ?  \n",
      "3       ?     ?    ?           ?  \n",
      "4       ?     ?    ?           ?  \n",
      "...   ...   ...  ...         ...  \n",
      "6139    3   319    3           ?  \n",
      "6140    1  2675    2           ?  \n",
      "6141    ?   473    3           ?  \n",
      "6142    ?     ?    ?           ?  \n",
      "6143    ?     ?    ?           ?  \n",
      "\n",
      "[6144 rows x 26 columns]\n"
     ]
    }
   ],
   "source": [
    "# Replacing the missing values with a question mark(?)\n",
    "\n",
    "preprocessing_dataset = preprocessing_dataset.fillna('?')\n",
    "print(preprocessing_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Year           0\n",
      "Mo             0\n",
      "Dy             0\n",
      "Hr             0\n",
      "Mn             0\n",
      "Sec            0\n",
      "Tsu            0\n",
      "Vol            0\n",
      "Addl           0\n",
      "Name           0\n",
      "Latitude       0\n",
      "Longitude      0\n",
      "Focal          0\n",
      "Mag            0\n",
      "MMI Int        0\n",
      "Num            0\n",
      "De             0\n",
      "Num.1          0\n",
      "De.1           0\n",
      "$Mill          0\n",
      "De.2           0\n",
      "Num.2          0\n",
      "De.3           0\n",
      "Num.3          0\n",
      "De.4           0\n",
      "Unnamed: 25    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Verifying that there are no missing values present in the entire dataset\n",
    "\n",
    "print(preprocessing_dataset.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Filtering required columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out of the 26 columns, Month, Day, Hour, Minute and Second that the earthquake occurred is thought to be not relevant for most of the analysis. They are referred to later when comparing the volcano and earthquake events. For the rest of the analysis, only the following columns are filtered: \n",
    "\n",
    "Year, Tsu, Vol, Name, Latitude, Longitude, Focal Depth, Mag, and MMI Int. The code for filtering these columns is displayed below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Year  Tsu  Vol                              Name Latitude Longitude  \\\n",
      "0    -2150    ?    ?      JORDAN: BAB-A-DARAA,AL-KARAK     31.1      35.5   \n",
      "1    -2000  Tsu    ?                     SYRIA: UGARIT   35.683      35.8   \n",
      "2    -2000    ?    ?                   TURKMENISTAN: W       38      58.2   \n",
      "3    -1610  Tsu  Vol  GREECE: THERA ISLAND (SANTORINI)     36.4      25.4   \n",
      "4    -1566    ?    ?           ISRAEL: ARIHA (JERICHO)     31.5      35.3   \n",
      "...    ...  ...  ...                               ...      ...       ...   \n",
      "6139  2019    ?    ?         PAKISTAN: MIRPUR DISTRICT   33.106    73.766   \n",
      "6140  2019    ?    ?          INDONESIA: MALUKU: AMBON    -3.45   128.347   \n",
      "6141  2019    ?    ?                  TURKEY: ISTANBUL    40.89    28.173   \n",
      "6142  2019    ?    ?              CHILE: SOUTH CENTRAL  -40.815   -72.002   \n",
      "6143  2019    ?    ?                 CHILE: CONCEPCION  -35.473   -73.162   \n",
      "\n",
      "      Mag MMI Int  \n",
      "0     7.3       ?  \n",
      "1       ?      10  \n",
      "2     7.1      10  \n",
      "3       ?       ?  \n",
      "4       ?      10  \n",
      "...   ...     ...  \n",
      "6139  5.6       ?  \n",
      "6140  6.5       ?  \n",
      "6141  5.7       ?  \n",
      "6142  6.1       ?  \n",
      "6143  6.8       ?  \n",
      "\n",
      "[6144 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "dataset_for_analysis = preprocessing_dataset.filter(['Year','Tsu', 'Vol','Name','Latitude','Longitude','Focal Depth', 'Mag','MMI Int'], axis=1)\n",
    "print(dataset_for_analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4. Adding required columns "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 'Name' column consists of Country and Region separated by \":\". Two new columns *Country* and *Region* are created by splitting the *Name* column into Country and Region for accurate analysis using the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Year  Tsu  Vol                              Name Latitude Longitude  \\\n",
      "0    -2150    ?    ?      JORDAN: BAB-A-DARAA,AL-KARAK     31.1      35.5   \n",
      "1    -2000  Tsu    ?                     SYRIA: UGARIT   35.683      35.8   \n",
      "2    -2000    ?    ?                   TURKMENISTAN: W       38      58.2   \n",
      "3    -1610  Tsu  Vol  GREECE: THERA ISLAND (SANTORINI)     36.4      25.4   \n",
      "4    -1566    ?    ?           ISRAEL: ARIHA (JERICHO)     31.5      35.3   \n",
      "...    ...  ...  ...                               ...      ...       ...   \n",
      "6139  2019    ?    ?         PAKISTAN: MIRPUR DISTRICT   33.106    73.766   \n",
      "6140  2019    ?    ?          INDONESIA: MALUKU: AMBON    -3.45   128.347   \n",
      "6141  2019    ?    ?                  TURKEY: ISTANBUL    40.89    28.173   \n",
      "6142  2019    ?    ?              CHILE: SOUTH CENTRAL  -40.815   -72.002   \n",
      "6143  2019    ?    ?                 CHILE: CONCEPCION  -35.473   -73.162   \n",
      "\n",
      "      Mag MMI Int       Country                     Region  \n",
      "0     7.3       ?        JORDAN       BAB-A-DARAA,AL-KARAK  \n",
      "1       ?      10         SYRIA                     UGARIT  \n",
      "2     7.1      10  TURKMENISTAN                          W  \n",
      "3       ?       ?        GREECE   THERA ISLAND (SANTORINI)  \n",
      "4       ?      10        ISRAEL            ARIHA (JERICHO)  \n",
      "...   ...     ...           ...                        ...  \n",
      "6139  5.6       ?      PAKISTAN            MIRPUR DISTRICT  \n",
      "6140  6.5       ?     INDONESIA                     MALUKU  \n",
      "6141  5.7       ?        TURKEY                   ISTANBUL  \n",
      "6142  6.1       ?         CHILE              SOUTH CENTRAL  \n",
      "6143  6.8       ?         CHILE                 CONCEPCION  \n",
      "\n",
      "[6144 rows x 10 columns]\n"
     ]
    }
   ],
   "source": [
    "dataset_for_analysis['Country'] = dataset_for_analysis.Name.str.split(':').str[0]\n",
    "dataset_for_analysis['Region'] = dataset_for_analysis.Name.str.split(':').str[1]\n",
    "print(dataset_for_analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5. Offsetting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 'Year' column ranges from -2150 to 2019 as verified by the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    6144.000000\n",
       "mean     1804.691243\n",
       "std       376.404999\n",
       "min     -2150.000000\n",
       "25%      1820.000000\n",
       "50%      1928.000000\n",
       "75%      1988.000000\n",
       "max      2019.000000\n",
       "Name: Year, dtype: float64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_for_analysis['Year'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The analysis of this dataset is aimed at splitting the dataset into 1000-year sets to observe trends within and between each set. In order to do this, 2150 is added to every row in *Year* to make the partitioning easier. This way, *Year* now ranges from 0 to 4160, making it easier to partition during the analysis. Offsetting is demonstrated in the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Year  Tsu  Vol                              Name Latitude Longitude  \\\n",
      "0    -2150    ?    ?      JORDAN: BAB-A-DARAA,AL-KARAK     31.1      35.5   \n",
      "1    -2000  Tsu    ?                     SYRIA: UGARIT   35.683      35.8   \n",
      "2    -2000    ?    ?                   TURKMENISTAN: W       38      58.2   \n",
      "3    -1610  Tsu  Vol  GREECE: THERA ISLAND (SANTORINI)     36.4      25.4   \n",
      "4    -1566    ?    ?           ISRAEL: ARIHA (JERICHO)     31.5      35.3   \n",
      "...    ...  ...  ...                               ...      ...       ...   \n",
      "6139  2019    ?    ?         PAKISTAN: MIRPUR DISTRICT   33.106    73.766   \n",
      "6140  2019    ?    ?          INDONESIA: MALUKU: AMBON    -3.45   128.347   \n",
      "6141  2019    ?    ?                  TURKEY: ISTANBUL    40.89    28.173   \n",
      "6142  2019    ?    ?              CHILE: SOUTH CENTRAL  -40.815   -72.002   \n",
      "6143  2019    ?    ?                 CHILE: CONCEPCION  -35.473   -73.162   \n",
      "\n",
      "      Mag MMI Int       Country                     Region  UpdatedYear  \n",
      "0     7.3       ?        JORDAN       BAB-A-DARAA,AL-KARAK            0  \n",
      "1       ?      10         SYRIA                     UGARIT          150  \n",
      "2     7.1      10  TURKMENISTAN                          W          150  \n",
      "3       ?       ?        GREECE   THERA ISLAND (SANTORINI)          540  \n",
      "4       ?      10        ISRAEL            ARIHA (JERICHO)          584  \n",
      "...   ...     ...           ...                        ...          ...  \n",
      "6139  5.6       ?      PAKISTAN            MIRPUR DISTRICT         4169  \n",
      "6140  6.5       ?     INDONESIA                     MALUKU         4169  \n",
      "6141  5.7       ?        TURKEY                   ISTANBUL         4169  \n",
      "6142  6.1       ?         CHILE              SOUTH CENTRAL         4169  \n",
      "6143  6.8       ?         CHILE                 CONCEPCION         4169  \n",
      "\n",
      "[6144 rows x 11 columns]\n"
     ]
    }
   ],
   "source": [
    "UpdatedYear = dataset_for_analysis['Year'] + 2150\n",
    "#print(UpdatedYear)\n",
    "dataset_for_analysis['UpdatedYear'] = UpdatedYear\n",
    "print(dataset_for_analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description of the columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Year     : The year in which earthquake occurred. It ranges from -2150 to 2019.\n",
    "    Tsu      : Occurrence of tsunami\n",
    "    Vol      : Occurrence of a volcano\n",
    "    Name     : Place where the earthquake occurred\n",
    "    Latitude : Latitude of earthquake occurrence\n",
    "    Longitude: Longitude of the earthquake occurrence\n",
    "    Mag      : Magntiude of the earthquake\n",
    "    MMI Int  : Measure of damage caused by the earthquake"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6. Downloading preprocessed dataset as a csv file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code downloads the preprocessed dataset as a csv file onto the local system. The downloaded file is then used for further analysis from here on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_for_analysis.to_csv('Dataset For Analysis.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.7. Insights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview of the dataset\n",
    "\n",
    "    1. The dataset consists of 26 columns and 6144 rows.\n",
    "\n",
    "### Addressing missing values\n",
    "\n",
    "    1. There exists one missing value in *Name* column. \n",
    "    2. The latitude and longitude did not match with any other record in the dataset, indicating a unique event.\n",
    "    3. Google Maps was then used to identify the location as Cook Strait in New Zealand.\n",
    "    4. The database was updated accordingly and verified to make sure the null value was updated.\n",
    "    5. The missing values in latitude and longitude were replaced with 0.0 in accordance with NOAA's definitions. \n",
    "    6. The reasons behind the missing values in other variables is not well understood and is replaced with a '?'.\n",
    "    \n",
    "    \n",
    "### Filtering and adding required columns\n",
    "\n",
    "    1. Out of the entire dataset, eight columns: Year, Tsu, Vol, Name, Latitude, Longitude, Magnitude, MMI Int are filtered.\n",
    "    2. 'Name' is of the format 'Country:Region'. It is split accordingly and two new columns are created for Country and Region\n",
    "    3. The dataset now consists of 10 columns\n",
    "    \n",
    "### Offsetting\n",
    "\n",
    "    1. The Year column ranges from -2150 to 2019. The negative sign indicates B.C. \n",
    "    2. In further analysis, the dataset will be partioned into subsets for detail exploration. \n",
    "    3. To do this, a new column UpdatedYear is creating by adding 2150 to every row in Year column.\n",
    "    4. The UpdatedYear now ranges from 0 to 4169. \n",
    "    5. The dataset now consists of 11 columns. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
